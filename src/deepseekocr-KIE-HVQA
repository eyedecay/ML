import torch
from transformers import AutoModel, AutoTokenizer
import os
import csv
import json


#CONFIG

MODEL = "deepseek-ai/DeepSeek-OCR"
DATA_FILE = "../KIE-HVQA/kie_hocr.jsonl"
IMAGE_BASE_PATH = "../KIE-HVQA"

OUTPUT_CSV = "kie_hvqa_deepseek_ocr_results.csv"
OUTPUT_JSONL = "kie_hvqa_deepseek_ocr_results.jsonl"
OUTPUT_DIR = "./outputs/kie_hvqa"

os.environ["CUDA_VISIBLE_DEVICES"] = '0'
os.makedirs(OUTPUT_DIR, exist_ok=True)
if os.path.exists(OUTPUT_JSONL):
    os.remove(OUTPUT_JSONL)


#functions

def save_result_incrementally_csv(result, file_path):
    """
    saves results incrementally
    Args:
        file_path
    """
    file_exists = os.path.isfile(file_path)
    with open(file_path, 'a', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=result.keys())
        if not file_exists:
            writer.writeheader()
        writer.writerow(result)

def save_result_incrementally_jsonl(result, file_path):
    """
    saves results incrementally (JSON)
    Args:
        file_ath
    """
    with open(file_path, 'a', encoding='utf-8') as f:
        f.write(json.dumps(result, ensure_ascii=False) + '\n')

#System Check
print("SYSTEM CHECK")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU Device: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

#Load Model
print(f"\nLoading {MODEL}...")
tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)
model = AutoModel.from_pretrained(MODEL, trust_remote_code=True, torch_dtype=torch.bfloat16)
model = model.eval().cuda()
print("Model loaded on GPU")


#Load Dataset
print(f"\nLoading dataset from {DATA_FILE}...")
dataset = []
with open(DATA_FILE, 'r', encoding='utf-8') as f:
    for line in f:
        if line.strip():
            dataset.append(json.loads(line))
print(f"✓ Dataset loaded: {len(dataset)} examples")

#Inference
total = len(dataset)
print("Starting evaluation on entire dataset...")

for idx, row in enumerate(dataset):
    print(f"\n[{idx+1}/{total}] Processing {row.get('id', 'unknown')}...")

    # Resolve image path
    image_path = row['image'].replace('./images/', './data/')
    if not os.path.isabs(image_path):
        image_path = os.path.join(IMAGE_BASE_PATH, image_path)
    image_path = os.path.normpath(image_path)

    if not os.path.exists(image_path):
        print(f"  ⚠ Warning: Image not found: {image_path}")
        continue

    question = row['problem'].replace("<image>", "").strip()
    ground_truth = row['answer']

    # Parse ground truth JSON if possible
    try:
        gt_parsed = json.loads(ground_truth)
    except:
        gt_parsed = {"Final OCR": ground_truth}

    prompt = "<image>\nFree OCR."

    try:
        print("  Running inference...")

        # Capture stdout to grab OCR text
        import sys
        from io import StringIO
        old_stdout = sys.stdout
        sys.stdout = captured = StringIO()

        res = model.infer(
            tokenizer,
            prompt=prompt,
            image_file=image_path,
            output_path=OUTPUT_DIR,
            base_size=1024,
            image_size=640,
            crop_mode=True,
            save_results=False,
            test_compress=False
        )

        sys.stdout = old_stdout
        captured_text = captured.getvalue()

        #Extract Text

        lines = captured_text.split("\n")
        ocr_text = []
        separator_count = 0
        collect = False

        for line in lines:
            if line.strip().startswith("===="):
                separator_count += 1
                if separator_count == 2:
                    collect = True
                elif separator_count == 3:
                    break
                continue
            if collect:
                if line.strip() == "":
                    continue
                ocr_text.append(line.rstrip())

        generated_text = "\n".join(ocr_text).strip()
        if not generated_text:
            generated_text = "[ERROR: OCR text not found]"

        print(" Generated OCR:")
        print(generated_text[:300])

    except Exception as e:
        sys.stdout = old_stdout
        print(f"  ✗ Error during inference: {e}")
        import traceback
        traceback.print_exc()
        generated_text = f"[ERROR: {str(e)}]"

    #Save
    pred_parsed = {
        "clear Char-level OCR": generated_text,
        "not clear enough Char-level OCR": "",
        "Final OCR": generated_text
    }

    csv_result = {
        "idx": idx,
        "id": row.get('id', 'unknown'),
        "image_path": image_path,
        "question": question,
        "ground_truth_raw": ground_truth,
        "ground_truth_final": gt_parsed.get("Final OCR", ""),
        "generated_text": generated_text
    }
    save_result_incrementally_csv(csv_result, OUTPUT_CSV)

    jsonl_result = {
        "answer": f"<answer>{ground_truth}</answer>",
        "response": f"<answer>{json.dumps(pred_parsed, ensure_ascii=False)}</answer>"
    }
    save_result_incrementally_jsonl(jsonl_result, OUTPUT_JSONL)

    # Clear GPU memory every 10 iterations
    if idx % 10 == 0:
        torch.cuda.empty_cache()

print("Evaluation complete on entire dataset")
print(f"CSV: {OUTPUT_CSV}")
print(f"JSONL: {OUTPUT_JSONL}")
print("\nRun evaluation:")
print(f"  cd ../KIE-HVQA && python eval.py --input_file ../src/{OUTPUT_JSONL}")
