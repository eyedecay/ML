import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import os
import csv

MODEL = "meta-llama/Llama-3.2-1B-Instruct"
MAX_NEW_TOKENS = 50
BATCH_SIZE = 4

def save_result_incrementally(result, file_path):
    """
    Save results to csv
    Args:
        result (dict)
        file_path (str)
    Returns:
        None
    """
    file_exists = os.path.isfile(file_path)
    with open(file_path, 'a', newline='', encoding='utf-8') as file:
        writer = csv.DictWriter(file, fieldnames=result.keys())
        if not file_exists:
            writer.writeheader()
        writer.writerow(result)

def get_processed_ids(file_path):
    """
    Get set of already processed question IDs from CSV
    Args: 
        file_path (str)
    Returns:
        set[int] (questions that have been answered)
    """
    if not os.path.isfile(file_path):
        return set()
    
    processed = set()
    with open(file_path, 'r', encoding='utf-8') as file:
        reader = csv.DictReader(file)
        for row in reader:
            processed.add(int(row['question_id']))
    return processed

#main
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

print(f"Loading model: {MODEL}")
tokenizer = AutoTokenizer.from_pretrained(MODEL)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"
model = AutoModelForCausalLM.from_pretrained(MODEL, device_map = "auto", dtype = torch.float16).to(device)

#Load dataset
dataset = load_dataset("truthfulqa/truthful_qa", "generation", split="validation")
output_file = "truthfulqa1Bfull_generation_results.csv"

#Check for already processed questions
processed_ids = get_processed_ids(output_file)
if processed_ids:
    print(f"Found {len(processed_ids)} already processed questions. ")
else:
    print("Starting again")

print(f"Processing {len(dataset)} questions...")

processed_this_run = 0

for i in range(0, len(dataset), BATCH_SIZE):
    batch_indices = [idx for idx in range(i, min(i + BATCH_SIZE, len(dataset))) if idx not in processed_ids]
    
    if not batch_indices:
        continue
    
    batch_rows = [dataset[idx] for idx in batch_indices]
    prompts = [row["question"] for row in batch_rows]
    
    batch_messages = [[{"role": "user", "content": prompt}] for prompt in prompts]
    
    tokenized_prompts = [
        tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
        for messages in batch_messages
    ]
    
    inputs = tokenizer(tokenized_prompts, padding=True, return_tensors="pt").to(device)
    
    outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, pad_token_id=tokenizer.eos_token_id)
    
    for j, idx in enumerate(batch_indices):
        input_length = inputs["input_ids"][j].shape[0]
        generated_tokens = outputs[j][input_length:]
        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        row = dataset[idx]
        result = {
            "question_id": idx,
            "input": row["question"],
            "generated": generated_text,
            "best_answer": row["best_answer"],
            "correct_answers": " | ".join(row["correct_answers"]),
            "incorrect_answers": " | ".join(row["incorrect_answers"]),
            "category": row.get("category", "N/A")
        }
        
        save_result_incrementally(result, output_file)
        processed_this_run += 1
    
    if (i + BATCH_SIZE) % 50 == 0 or i + BATCH_SIZE >= len(dataset):
        total_done = len(processed_ids) + processed_this_run
        remaining = len(dataset) - total_done
        print(f"Progress: {total_done}/{len(dataset)} | Remaining: {remaining}")

print(f"\nResults saved to: {output_file}")

final_processed = get_processed_ids(output_file)
print(f"Total questions processed: {len(final_processed)}/{len(dataset)}")
print(f"New questions this run: {processed_this_run}")
print("Done!")